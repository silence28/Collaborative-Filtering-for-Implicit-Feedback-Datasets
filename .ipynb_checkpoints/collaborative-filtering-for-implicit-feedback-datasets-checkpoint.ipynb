{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaborative Filtering for Implicit Feedback Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Article for the math behind the code: http://yifanhu.net/PUB/cf.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the data:\n",
    "\n",
    "The code implemented for collaborative filtering for implicit feedback needs some kind of interaction between the user and the item. Types of implicit feedback include purchase history, browsing history, search patterns, or even mouse movements. For example, a user that purchased many books by the same author probably likes that author.\n",
    "\n",
    "The data used here is from MovieLens with 100.000 recommendations from 943 users who have rated 1682 movies (Items).\n",
    "In this example, the rating is considered as an interaction, so the interaction could be a fraction of the video that has been watched.  \n",
    "\n",
    "Here we pretend like the interaction is the number of times the user has clicked on the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User  Item  Click\n",
      "0   196   242      3\n",
      "1   186   302      3\n",
      "2    22   377      1\n",
      "3   244    51      2\n",
      "4   166   346      1\n"
     ]
    }
   ],
   "source": [
    "path = 'u.data' # data path\n",
    "df = pd.read_csv(path, sep='\\t', names=['User', 'Item', 'Click', 'Timestamp'], header=None)\n",
    "df = df.drop('Timestamp', axis=1) # Removing Timestamp\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sort the df by User, Item, and Click"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The users are sorted according to the items:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of movies watch by the users = 300000\n",
      "\n",
      "       User  Item  Click\n",
      "32236     1     1      5\n",
      "23171     1     2      3\n",
      "83307     1     3      4\n",
      "62631     1     4      3\n",
      "47638     1     5      3\n"
     ]
    }
   ],
   "source": [
    "df_Sorted = df.sort_values(['User', 'Item', 'Click'])\n",
    "print(\"Total number of movies watch by the users = {}\\n\".format(df_Sorted.size))\n",
    "print(df_Sorted.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop the duplicated records (If someone watched the same item twice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of clean_df = 300000\n",
      "\n",
      "       User  Item  Click\n",
      "32236     1     1      5\n",
      "23171     1     2      3\n",
      "83307     1     3      4\n",
      "62631     1     4      3\n",
      "47638     1     5      3\n"
     ]
    }
   ],
   "source": [
    "clean_df = df_Sorted.drop_duplicates(['User', 'Item'], keep = 'last')\n",
    "print(\"Size of clean_df = {}\\n\".format(clean_df.size))\n",
    "print(clean_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users and 1682 items in the data\n"
     ]
    }
   ],
   "source": [
    "n_User = len(clean_df.User.unique())\n",
    "n_Item = len(clean_df.Item.unique())\n",
    "\n",
    "print('There are {0} users and {1} items in the data'.format(n_User, n_Item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we build a matrix of Users x Items, how many cells in the matrix will be filled?\n",
    "Fraction of cells which is filled (Sparsity):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.30% of the user-item matrix is filled\n"
     ]
    }
   ],
   "source": [
    "sparsity = clean_df.shape[0] / float(n_User * n_Item)\n",
    "print('{:.2%} of the user-item matrix is filled'.format(sparsity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Item preference matrix:\n",
    "He we make a matrix which tells us which movie has been seen by the user. If the movie is watched by a user: \n",
    "\n",
    "\\begin{equation}\n",
    "p_{ui} =    \\begin{cases}\n",
    "    1, & r_{ui} > 0.\\\\\n",
    "    0, & r_{ui} = 0.\n",
    "  \\end{cases}\n",
    "  \\end{equation}\n",
    " \n",
    "$r_{ui}$: user $u$ clicked(or other interaction) number of times on item $i$\n",
    "\n",
    "$p_{ui}$: user $u$ consumed item $i$ $(r_{ui} > 0)$, then we have an indication that $u$ likes $i$ $(p_{ui} = 1)$.\n",
    "On the other hand, if $u$ never consumed $i$, we believe no preference $(p_{ui} = 0)$. \n",
    "\n",
    "The preference matrix $p_{ui}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single user preference on each items: [[1. 1. 1. ... 0. 0. 0.]]\n",
      "Shape of the User_Item_pref matrix: (943, 1682)\n"
     ]
    }
   ],
   "source": [
    "User_Item_pref = clean_df.copy()\n",
    "User_Item_pref['Click'][User_Item_pref['Click'] > 0] = 1  \n",
    "User_Item_pref = User_Item_pref.pivot(index='User', columns='Item', values='Click')\n",
    "User_Item_pref.fillna(0, inplace=True)\n",
    "User_Item_pref = User_Item_pref.values\n",
    "\n",
    "print(\"Single user preference on each items: {}\".format(User_Item_pref[0:1])) \n",
    "print(\"Shape of the User_Item_pref matrix: {}\".format(User_Item_pref.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### User Item interaction matrix:\n",
    "User_Item_interactions: matrix where we can see the number of clicks for each user $r_{ui}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single user clicks on each items: [[4. 0. 0. ... 0. 0. 0.]]\n",
      "Shape of the User_Item_interactions matrix: (943, 1682)\n"
     ]
    }
   ],
   "source": [
    "User_Item_interactions = clean_df.pivot(index='User', columns='Item', values='Click')\n",
    "User_Item_interactions.fillna(0, inplace=True)\n",
    "User_Item_interactions = User_Item_interactions.values\n",
    "print(\"Single user clicks on each items: {}\".format(User_Item_interactions[1:2])) \n",
    "print(\"Shape of the User_Item_interactions matrix: {}\".format(User_Item_interactions.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "943 users viewed 20 or more items\n"
     ]
    }
   ],
   "source": [
    "k = 10 # Number of top k items we want to recommend for the user\n",
    "\n",
    "# View_counts counts the number of item viewed by each user:\n",
    "View_counts = np.apply_along_axis(np.bincount, 1, User_Item_pref.astype(int))\n",
    "\n",
    "# buyers_idx finds the users who seen 2*k movies/items:\n",
    "buyers_idx = np.where(View_counts[:, 1] >= k*2)[0] \n",
    "print('{0} users viewed {1} or more items'.format(len(buyers_idx), k*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly selected users for test set : [441 707 232 369 247  47 383 594 283 381 792 378 543 923  28 114 311 325\n",
      " 464 702 613 102 933 838 925 679 524 181  39  21  13 596 243 674 852 746\n",
      " 772 450 787 593 467 940 668 671  82 851 189 741 664 242 324 698 689 284\n",
      " 576 844 505 935 666 299 161 795 871  36  88 570 848 927 166 605 789 588\n",
      " 128 521 887 423 811  34 287 697 110 744 330   0 774 906 693 565 326 647\n",
      "  32  41 394 759 812]\n"
     ]
    }
   ],
   "source": [
    "test_frac = 0.2 \n",
    "test_users_idx = np.random.choice(buyers_idx,\n",
    "                                  size = int(np.ceil(len(buyers_idx) * test_frac)),\n",
    "                                  replace = False)\n",
    "\n",
    "val_users_idx = test_users_idx[:int(len(test_users_idx) / 2)]\n",
    "test_users_idx = test_users_idx[int(len(test_users_idx) / 2):]\n",
    "print(\"Randomly selected users for test set : {}\".format(test_users_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A function used to mask the preferences data from training matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(interaction, dat, train, test, user_idx, k):\n",
    "    for user in user_idx:\n",
    "        purchases = np.where(dat[user, :] == 1)[0]\n",
    "        mask = np.random.choice(purchases, size = k, replace = False)\n",
    "        interaction[user, mask] = 0\n",
    "        train[user, mask] = 0\n",
    "        test[user, mask] = dat[user, mask]\n",
    "    return train, test, interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_matrix = np.zeros(shape = (n_User, n_Item))\n",
    "train_matrix = User_Item_pref.copy()\n",
    "test_matrix = zero_matrix.copy()\n",
    "val_matrix = zero_matrix.copy()\n",
    "\n",
    "# Mask the train matrix and create the validation and test matrices\n",
    "train_matrix, val_matrix, User_Item_interactions = data_process(User_Item_interactions,User_Item_pref, train_matrix, val_matrix, val_users_idx, k)\n",
    "train_matrix, test_matrix, User_Item_interactions = data_process(User_Item_interactions,User_Item_pref, train_matrix, test_matrix, test_users_idx, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(943, 1682)\n",
      "(943, 1682)\n"
     ]
    }
   ],
   "source": [
    "print(train_matrix.shape)\n",
    "print(val_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# let's take a look at what was actually accomplised\n",
    "# You can see the test matrix preferences are masked in the train matrix\n",
    "test_matrix[      test_users_idx[0]  , test_matrix[test_users_idx[0], :].nonzero()[0]   ]\n",
    "print(train_matrix[test_users_idx[0], test_matrix[test_users_idx[0], :].nonzero()[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow\n",
    "Important: Several of the hyperparameters should be optimized by grid search!\n",
    "\n",
    "To understand the math read at the article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() # Create a new graphs\n",
    "\n",
    "pref = tf.placeholder(tf.float32, (n_User, n_Item))  # Here's the preference matrix\n",
    "interactions = tf.placeholder(tf.float32, (n_User, n_Item), name = 'interactions') # Here viewed or not viewed matrix\n",
    "users_idx = tf.placeholder(tf.int32, (None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_features: Number of latent features to be extracted. (Hyperparameter)\n",
    "n_features = 10 \n",
    "\n",
    "# The X matrix represents the user latent preferences with a shape of user x latent features\n",
    "X = tf.Variable(tf.truncated_normal([n_User, n_features], mean = 0, stddev = 0.05))\n",
    "\n",
    "# The Y matrix represents the item latent features with a shape of item x latent features\n",
    "Y = tf.Variable(tf.truncated_normal([n_Item, n_features], mean = 0, stddev = 0.05))\n",
    "\n",
    "# Here's the initilization of the confidence parameter \n",
    "conf_alpha = tf.Variable(tf.random_uniform([1], 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a user bias vector n_User, n_Item\n",
    "user_bias = tf.Variable(tf.truncated_normal([n_User, 1], stddev = 0.2))\n",
    "\n",
    "# Concatenate the vector to the user matrix\n",
    "# Due to how matrix algebra works, we also need to add a column of ones to make sure\n",
    "# the resulting calculation will take into account the item biases.\n",
    "X_plus_bias = tf.concat([X, \n",
    "                         #tf.convert_to_tensor(user_bias, dtype = tf.float32),\n",
    "                         user_bias,\n",
    "                         tf.ones((n_User, 1), dtype = tf.float32)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the item bias vector\n",
    "item_bias = tf.Variable(tf.truncated_normal([n_Item, 1], stddev = 0.2))\n",
    "\n",
    "# Cocatenate the vector to the item matrix\n",
    "# Also, adds a column one for the same reason stated above.\n",
    "Y_plus_bias = tf.concat([Y, \n",
    "                         tf.ones((n_Item, 1), dtype = tf.float32),\n",
    "                         item_bias],\n",
    "                         axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we finally multiply the matrices together to estimate the predicted preferences\n",
    "pred_pref = tf.matmul(X_plus_bias, Y_plus_bias, transpose_b=True, name ='pred_pref')\n",
    "\n",
    "# Construct the confidence matrix with the clicks and alpha paramter\n",
    "conf = 1 + conf_alpha * interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.multiply(conf, tf.square(tf.subtract(pref, pred_pref))))\n",
    "l2_sqr = tf.nn.l2_loss(X) + tf.nn.l2_loss(Y) + tf.nn.l2_loss(user_bias) + tf.nn.l2_loss(item_bias)\n",
    "\n",
    "lambda_c = 0.01\n",
    "loss = cost + lambda_c * l2_sqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.05\n",
    "optimize = tf.train.AdagradOptimizer(learning_rate = lr).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that helps to calculate the top k item average predicted precisions\n",
    "def top_k_precision(predicted, mat, k, user_idx):\n",
    "    \"\"\"The predicted is a (user x items) matrix is the probability for each \n",
    "    item.\"\"\"\n",
    "    precisions = []\n",
    "    \n",
    "    for user in user_idx:\n",
    "        rec = np.argsort(-predicted[user, :]) # The argsort sorts the recommendation\n",
    "                                            #for each user from high to low\n",
    "        top_k = rec[:k] # Getting the top k items\n",
    "        labels = mat[user, :].nonzero()[0]\n",
    "        precision = len(set(top_k) & set(labels)) / float(k) # Calculate the precisions from actual labels\n",
    "        precisions.append(precision)\n",
    "    return np.mean(precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Iterations 0...', 'Training Loss 294018.22...', 'Train Precision 0.136...', 'Val Precision 0.019')\n",
      "('Iterations 10...', 'Training Loss 110661.61...', 'Train Precision 0.499...', 'Val Precision 0.077')\n",
      "('Iterations 20...', 'Training Loss 90803.37...', 'Train Precision 0.547...', 'Val Precision 0.095')\n",
      "('Iterations 30...', 'Training Loss 79382.07...', 'Train Precision 0.573...', 'Val Precision 0.100')\n",
      "('Iterations 40...', 'Training Loss 69971.75...', 'Train Precision 0.584...', 'Val Precision 0.102')\n",
      "('Iterations 50...', 'Training Loss 61293.27...', 'Train Precision 0.587...', 'Val Precision 0.102')\n",
      "('Iterations 60...', 'Training Loss 52618.12...', 'Train Precision 0.585...', 'Val Precision 0.103')\n",
      "Shape of the recommendation matrix : (943, 1682)\n",
      "\n",
      "The average test recommendation precision for all users = 10.11%\n",
      "Recommendations for user 2 = [285 300 312 268 677 747 288 878 689 258]\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "iterations = 70\n",
    "#------\n",
    "User_id_for_recommendation = tf.Variable(2, name='User_id_for_recommendation')\n",
    "k_recommendations = tf.Variable(10, name='k_recommendations')\n",
    "\n",
    "# ------\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        sess.run(optimize, feed_dict = {pref: train_matrix,\n",
    "                                        interactions: User_Item_interactions})\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            mod_loss = sess.run(loss, feed_dict = {pref: train_matrix,\n",
    "                                                   interactions: User_Item_interactions})\n",
    "            mod_pred = pred_pref.eval()\n",
    "            train_precision = top_k_precision(mod_pred, train_matrix, k, val_users_idx)\n",
    "            val_precision = top_k_precision(mod_pred, val_matrix, k, val_users_idx)\n",
    "            print('Iterations {0}...'.format(i),\n",
    "                  'Training Loss {:.2f}...'.format(mod_loss),\n",
    "                  'Train Precision {:.3f}...'.format(train_precision),\n",
    "                  'Val Precision {:.3f}'.format(val_precision)\n",
    "                )\n",
    "\n",
    "    recommendation = pred_pref.eval() # This produce a matrix where the rows are the user and \n",
    "                                      #the columns are the expected recommendation for each movie\n",
    "    print(\"Shape of the recommendation matrix : {}\\n\".format(recommendation.shape))\n",
    "    test_precision = top_k_precision(recommendation, test_matrix, k, test_users_idx)\n",
    "    print('The average test recommendation precision for all users = {:.2f}%'.format(test_precision*100))\n",
    "    \n",
    "    # This part recommends items to a user:\n",
    "    rec_items = tf.contrib.framework.argsort(recommendation, -1, 'DESCENDING')\n",
    "    purchase_history = tf.transpose(tf.where(train_matrix[User_id_for_recommendation.eval(), :] != 0))\n",
    "    recommendations = rec_items[User_id_for_recommendation.eval(), :]\n",
    "    a = recommendations.eval().astype(np.int64)\n",
    "    b = purchase_history.eval().astype(np.int64)[0]\n",
    "    a0 = tf.expand_dims(a, 1)\n",
    "    b0 = tf.expand_dims(b, 0)\n",
    "    tensor = a\n",
    "    mask = ~tf.reduce_any(tf.equal(a0, b0), 1)\n",
    "    rec_for_user_id =  tf.boolean_mask(tensor, mask)[:k_recommendations.eval()]\n",
    "\n",
    "    # Giving a name to the recommendations_for_user_id tensor so it can be restored from the model:\n",
    "    recommendations_for_user_id = tf.identity(rec_for_user_id, name=\"recommendations_for_user_id\")\n",
    "\n",
    "    tf.train.write_graph(sess.graph.as_graph_def(), \"./model/\", 'graph.pbtxt', as_text=True)\n",
    "    print(\"Recommendations for user {} = {}\".format(User_id_for_recommendation.eval(), recommendations_for_user_id.eval()))\n",
    "    saver.save(sess, \"./modello/my_test_model\", global_step=1000)\n",
    "\n",
    "    # close the training session now that we've evaluated the output\n",
    "    sess.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restoring the model\n",
    "\n",
    "Remember for deploying the model in google cloud you should use 'tf.saved_model.simple_save' not 'tf.train.Saver( )'. But for saving the model locally 'tf.train.Saver( )' is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "sess=tf.Session()    \n",
    "saver = tf.train.import_meta_graph('model/my_test_model-1000.meta')\n",
    "saver.restore(sess,tf.train.latest_checkpoint('model/'))\n",
    "\n",
    "User_Id = 2 # The user id \n",
    "K_Recommendation = 10 # Top K recommendation for the user\n",
    "\n",
    "# Now, let's access and create placeholders variables and\n",
    "# create feed-dict to feed new data\n",
    "graph = tf.get_default_graph()\n",
    "k_recommendations = graph.get_tensor_by_name(\"k_recommendations:0\")\n",
    "User_id_for_recommendation = graph.get_tensor_by_name(\"User_id_for_recommendation:0\")\n",
    "feed_dict ={k_recommendations:K_Recommendation,User_id_for_recommendation:User_Id}\n",
    "\n",
    "#Now, access the op that you want to run. \n",
    "op_to_restore = graph.get_tensor_by_name(\"recommendations_for_user_id:0\")\n",
    "\n",
    "# This will print the recommendation for the user 2\n",
    "print sess.run(op_to_restore,feed_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
